{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1\n",
    "\n",
    "In a Multi-layer feedforward neural network with $L$ layers, each layer $l$ with $n_{l}$ neurons.\n",
    "each neuron has output:\n",
    "\n",
    "$a_{i}(l) = h(z_{i}(l))$ , where $z_{i}(l) = \\sum_{j=1}^{n_{l}-1}[w_{i,j}a_{j}(l-1) + b_{i}(l)], i=1,...,n_{l}, l= 2,...,L$ ......................................(1)\n",
    "\n",
    "where h is an activation function that can be (relu, sigmoid, softmax, linear, tanh, swish,...).\n",
    "\n",
    "As information enters the network and propagates forward, we eventually get an output $a(L)$ at the final layer of the network. And so measuring the error of the network's prediction w.r.t the known real target values $r$ for input $x$, using the mean squared error is given by the following:\n",
    "\n",
    "$E = \\frac{1}{2}||r - a(L)||_{2}^{2}$ ..................................................(2)\n",
    "\n",
    "Since machine learning is essentially just optimization, we use the above error as our objective function to be minimized, by changing the network parameters  in a smart way. This great tool is backpropagation of the error signal to all the layers of the network to inform the weights by how much they need to change in order for the loss to decrease. Since $E$ is a compound function in $z$ as well, we make use of the chain rule to calculate the derivative of the loss w.r.t the weights for the hidden layers, starting with the final layer and going back:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial z_{j}(L)} = \\frac{\\partial E}{\\partial a_{j}(L)} \\frac{\\partial a_{j}(L)}{\\partial z_{j}(L)}$\n",
    "\n",
    "with: $\\frac{\\partial E}{\\partial a_{j}(L)} = -2 \\times \\frac{1}{2}(r_{j} - a_{j}(L)) = a_{j}(L) - r_{j} $ using the MSE loss\n",
    "\n",
    "and: $\\frac{\\partial a_{j}(L)}{\\partial z_{j}(L)} = h(z_{j}(L))(1 - h(z_{j}(L)))$ using the sigmoid activation in the last layer, and so we have that\n",
    "\n",
    "$\\frac{\\partial E}{\\partial z_{j}(L)} = h(z_{j}(L)) (1 - h(z_{j}(L))) (a_{j}(L) - r_{j})$ .....................................(3)\n",
    "\n",
    "\n",
    "this only gives us the formula for derivatives in the final layer, to find the derivatives of the loss in the second last layer of the network, we have\n",
    "\n",
    "\n",
    "$\\frac{\\partial E}{\\partial z_{j}(L-1)} = \\sum_{i}^{n_{L}} \\frac{\\partial E}{\\partial a_{i}(L)} \\frac{\\partial a_{i}(L)}{\\partial z_{i}(L)} \\frac{\\partial z_{i}(L)}{\\partial a_{j}(L-1)} \\frac{\\partial a_{j}(L-1)}{\\partial z_{j}(L-1)} = \\frac{\\partial E}{\\partial z_{i}(L)} \\frac{\\partial z_{i}(L)}{\\partial a_{j}(L-1)} \\frac{\\partial a_{j}(L-1)}{\\partial z_{j}(L-1)} $\n",
    "\n",
    "where the sum i is over all  $n_{L}$ neurons in the layer infront. this provides us a general formular for any layer $l$ to have:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial z_{j}(l)}  = \\sum_{i}^{n_{l+1}}\\frac{\\partial E}{\\partial z_{i}(l+1)} \\frac{\\partial z_{i}(l+1)}{\\partial a_{j}(l)} \\frac{\\partial a_{j}(l)}{\\partial z_{j}(l)}  =  \\frac{\\partial h(z_{j}(l))}{\\partial z_{j}(l)}\\sum_{i}^{n_{l+1}}\\frac{\\partial E}{\\partial z_{i}(l+1)} \\frac{\\partial z_{i}(l+1)}{\\partial a_{j}(l)}$\n",
    "\n",
    "(_we can take the parts that do not depend on i out of the summation_)\n",
    "\n",
    "from (1) above we get that  $ \\frac{\\partial z_{i}(l+1)}{\\partial a_{i}(l)} =  w_{i,j}(l+1)$ therefor we get :\n",
    "\n",
    "\n",
    "$\\frac{\\partial E}{\\partial z_{j}(l)} = \\frac{\\partial h(z_{j}(l))}{\\partial z_{j}(l)} \\sum_{i}^{n_{l+1}}w_{i,j}(l+1)\\frac{\\partial E}{\\partial z_{i}(l+1)}$ .....................................(4)\n",
    "\n",
    "Now finally, we can express the weight and biases interms or the chainrule derivation above by going one step into the nested function $z$ given in (1) as follows:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_{i,j}(l)} =  \\frac{\\partial E}{\\partial z_{i}(l)} \\frac{\\partial z_{i}(l)}{\\partial w_{i,j}(l)} = a_{j}(l-1)\\frac{\\partial E}{\\partial z_{i}(l)} $\n",
    "\n",
    "and :\n",
    "\n",
    "$\\frac{\\partial E}{\\partial b_{i}(l)} =  \\frac{\\partial E}{\\partial z_{i}(l)} \\frac{\\partial z_{i}(l)}{\\partial b_{i}(l)} = 1\\times\\frac{\\partial E}{\\partial z_{i}(l)} $\n",
    "\n",
    "where $times\\frac{\\partial E}{\\partial z_{i}(l)} $ is given by the formula in (4), and the initial value is in  the final layer in (3) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
