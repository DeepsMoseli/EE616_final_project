{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1\n",
    "\n",
    "In a Multi-layer feedforward neural network with $L$ layers, each layer $l$ with $n_{l}$ neurons.\n",
    "each neuron has output:\n",
    "\n",
    "$a_{i}(l) = h(z_{i}(l))$ , where $z_{i}(l) = \\sum_{j=1}^{n_{l}-1}[w_{i,j}a_{j}(l-1) + b_{i}(l)], i=1,...,n_{l}, l= 2,...,L$ ......................................(1)\n",
    "\n",
    "where h is an activation function that can be (relu, sigmoid, softmax, linear, tanh, swish,...).\n",
    "\n",
    "As information enters the network and propagates forward, we eventually get an output $a(L)$ at the final layer of the network. And so measuring the error of the network's prediction w.r.t the known real target values $r$ for input $x$, using the mean squared error is given by the following:\n",
    "\n",
    "$E = \\frac{1}{2}||r - a(L)||_{2}^{2}$ ..................................................(2)\n",
    "\n",
    "Since machine learning is essentially just optimization, we use the above error as our objective function to be minimized, by changing the network parameters  in a smart way. This great tool is backpropagation of the error signal to all the layers of the network to inform the weights by how much they need to change in order for the loss to decrease. Since $E$ is a compound function in $z$ as well, we make use of the chain rule to calculate the derivative of the loss w.r.t the weights for the hidden layers, starting with the final layer and going back:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial z_{j}(L)} = \\frac{\\partial E}{\\partial a_{j}(L)} \\frac{\\partial a_{j}(L)}{\\partial z_{j}(L)}$\n",
    "\n",
    "with: $\\frac{\\partial E}{\\partial a_{j}(L)} = -2 \\times \\frac{1}{2}(r_{j} - a_{j}(L)) = a_{j}(L) - r_{j} $ using the MSE loss\n",
    "\n",
    "and: $\\frac{\\partial a_{j}(L)}{\\partial z_{j}(L)} = h(z_{j}(L))(1 - h(z_{j}(L)))$ using the sigmoid activation in the last layer, and so we have that\n",
    "\n",
    "$\\frac{\\partial E}{\\partial z_{j}(L)} = h(z_{j}(L)) (1 - h(z_{j}(L))) (a_{j}(L) - r_{j})$ .....................................(3)\n",
    "\n",
    "\n",
    "this only gives us the formula for derivatives in the final layer, to find the derivatives of the loss in the second last layer of the network, we have\n",
    "\n",
    "\n",
    "$\\frac{\\partial E}{\\partial z_{j}(L-1)} = \\sum_{i}^{n_{L}} \\frac{\\partial E}{\\partial a_{i}(L)} \\frac{\\partial a_{i}(L)}{\\partial z_{i}(L)} \\frac{\\partial z_{i}(L)}{\\partial a_{j}(L-1)} \\frac{\\partial a_{j}(L-1)}{\\partial z_{j}(L-1)} = \\frac{\\partial E}{\\partial z_{i}(L)} \\frac{\\partial z_{i}(L)}{\\partial a_{j}(L-1)} \\frac{\\partial a_{j}(L-1)}{\\partial z_{j}(L-1)} $\n",
    "\n",
    "where the sum i is over all  $n_{L}$ neurons in the layer infront. this provides us a general formular for any layer $l$ to have:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial z_{j}(l)}  = \\sum_{i}^{n_{l+1}}\\frac{\\partial E}{\\partial z_{i}(l+1)} \\frac{\\partial z_{i}(l+1)}{\\partial a_{j}(l)} \\frac{\\partial a_{j}(l)}{\\partial z_{j}(l)}  =  \\frac{\\partial h(z_{j}(l))}{\\partial z_{j}(l)}\\sum_{i}^{n_{l+1}}\\frac{\\partial E}{\\partial z_{i}(l+1)} \\frac{\\partial z_{i}(l+1)}{\\partial a_{j}(l)}$\n",
    "\n",
    "(_we can take the parts that do not depend on i out of the summation_)\n",
    "\n",
    "from (1) above we get that  $ \\frac{\\partial z_{i}(l+1)}{\\partial a_{i}(l)} =  w_{i,j}(l+1)$ therefor we get :\n",
    "\n",
    "\n",
    "$\\frac{\\partial E}{\\partial z_{j}(l)} = \\frac{\\partial h(z_{j}(l))}{\\partial z_{j}(l)} \\sum_{i}^{n_{l+1}}w_{i,j}(l+1)\\frac{\\partial E}{\\partial z_{i}(l+1)}$ .....................................(4)\n",
    "\n",
    "Now finally, we can express the weight and biases interms or the chainrule derivation above by going one step into the nested function $z$ given in (1) as follows:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_{i,j}(l)} =  \\frac{\\partial E}{\\partial z_{i}(l)} \\frac{\\partial z_{i}(l)}{\\partial w_{i,j}(l)} = a_{j}(l-1)\\frac{\\partial E}{\\partial z_{i}(l)} $\n",
    "\n",
    "and :\n",
    "\n",
    "$\\frac{\\partial E}{\\partial b_{i}(l)} =  \\frac{\\partial E}{\\partial z_{i}(l)} \\frac{\\partial z_{i}(l)}{\\partial b_{i}(l)} = 1\\times\\frac{\\partial E}{\\partial z_{i}(l)} $\n",
    "\n",
    "where $times\\frac{\\partial E}{\\partial z_{i}(l)} $ is given by the formula in (4), and the initial value is in  the final layer in (3) \n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "X = \\begin{bmatrix}\n",
    "x_{1} & x_{1} &  ... & x_{1,n_{p}}\n",
    "\\end{bmatrix}  \\in R ^{n_{l}\\times 1}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "a(l) = \\begin{bmatrix}\n",
    "h(z_{1}(l)) & h(z_{2}(l)) &  ... & h(z_{n_{l}}(l)) \n",
    "\\end{bmatrix}^T \\in R ^{1 \\times n_{p}}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "where $x_{i}$ is has dimensionality $n$ for each pattern.\n",
    "\n",
    "The MSE\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.B\n",
    "\n",
    "### How to run the code\n",
    "\n",
    "To make running my code easy and generally follow the style mosetly used in the open source community, my code is in 4 files that sit in the same folder. 3 are just utility functions.:\n",
    "\n",
    "1. __fina_project.ipynb - main code that imports the utilities and  will run and show everything.__\n",
    "2. augment.py  - utility class that execused the rotations and adds them to training data in a certain way.\n",
    "3. data_gen.py - data generator for my neural network, it also performs some transformations on the image.\n",
    "4. visualization.py - utility for visualization of the learned convolution filters.\n",
    "\n",
    "\n",
    "To be able to run this, I suggest cloning my [repository from github](https://github.com/DeepsMoseli/EE616_final_project) and firing up the fina_project.ipynb. This is to ensure all paths are in place.\n",
    "_note only the original $119$ images are in the repo :)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.C\n",
    "\n",
    "For the task of dental radiography classification , I tried 2 CNN architectures of different sizes to finally get to my final solution. I also explored different ways of augmentation and their merits and my concerns with them, I will explains my final data preparation, my initial model, and my final submission and why I made this choice.\n",
    "\n",
    "\n",
    "\n",
    "### Data preperation and Augmentation.\n",
    "\n",
    "* I first crop the image from (748,512) to (748,500) to remove the image information.\n",
    "* I do not resize the images beyond this as I can not visibly see the desease so I have no prio to say the indicator is in the middle or edges of the image.\n",
    "* Rotation of images was perfomed only on the $119$ training images to grow the training dataset, and not on the $20$ test images.\n",
    "* While there was a need to perform rotations $[90,180,270]$ to increase the data, I noticed since the images are not square, this introduces black spaces around the image, I suspect this will add extra complexity in learning. I eventually only peformed $[0, 180]$ augmentation to increase and duplicate the minority class instead of non vertical rotation.\n",
    "* Also on ploting the images, some looked like they are actully rgb and some not, they had different colors in PIL, for example image $10$ and image $110$ i then applied a grayscale transform.\n",
    "* $88\\%$ of the original training data, and $85\\%$ of the test data  is class 1, this is a huge imbalance in the classes, especially as the data is already so small.\n",
    "* I decided to rotate class $1$ images with probability $0.3$, but always augment class $0$ images. This was to try and close the class imbalance while increasing the data.\n",
    "* To metigate the network just remembering the rotated images, I used a gaussian blur filter on the images, the radius values of the blur were randomly sampled from $U(0,4)$.\n",
    "* The data generator simply creates a pipeline from images to our model and applies a few transformations such as normalization. I cannot crop since I dont know where the objects that show disease sit.\n",
    "\n",
    "----\n",
    "### Deep learning Models\n",
    "\n",
    "* #### A  4 CNN + 4 pooling + 1 FC\n",
    "I first put together my own simple neural network with alternating convolution and pooling layers and a final fully connected layer and softmax layer. This network helped me in fully understanding the math of convolutional dimensions. While I could have continued and explored how to add more sophistication to my model such as skip connections and so on, I had a much stronger feeling the realproblem lies in the data  and not so much in a golden bullet model. So I quickly  decided on using transfer learning that I detail in the next section. This would allow me to spend more time in figuring the data.  Below is the model structure for my own implementation.\n",
    "\n",
    "![CNN implementation](CNN.jpg \"CNN implementation\")\n",
    "\n",
    "\n",
    "I use relu after each pooling layer and the fully connected layes and my final output is a softmax.(not the fully connected)\n",
    "\n",
    "This model failed to learn anything useful for a long time and changing of the training configurations such as learning rate, optimizer, regularizer, number of fully connected layers. It always seemed to just overfit the training data and then predict everything as class 1 in the test dataset, giving an accuracy of 85%, which is misleading. A better metric such as the ROC AUC score would reveal that this model isnt doing very well. and infact simply looking at the confusion matrix shows this. it has a $100%$ false positive rate.  I will now discuss the next architecture I used.\n",
    "\n",
    "-----\n",
    "\n",
    "* #### Transfer learning from Pretrained [Resnet34](https://pytorch.org/hub/pytorch_vision_resnet/)\n",
    "Due the limited training data we hace and the high dimensionality, the classification problem at hand is hard to solve by training a complex convolutional network with a large number of training parameters. We simply do not have enough data to learn salient features from the data present. To go around this, I use a pretrained convolutional network. This method involves using a network with learned (instead of random state) weights that represent salient features about images learned from a much larger dataset.\n",
    "\n",
    "I decided to use the 34 layer ResNet model, trained on  the ImageNet dataset and has weights stored in the pytorch hub that i can download. Resnest  was introduced in the paper [\"Deep Residual Learning for Image Recognition\"](https://arxiv.org/abs/1512.03385). I replaced the fully connected layer with a fullu connected with relu activation on 8 neurons and added a final softmax layer for the 2 classes. The pytorch pretrained models expects input images pf shape $(3 \\times H \\times W )$ where $H$ and $W$ are expected to be atleast 224. It does not requirethat we have square images.\n",
    "\n",
    "* In training this network, I used the ADAM optimizer with the default learning rate of $0.01$ and zero weight decay. \n",
    "* I played around with the dropout regularizer but found that it only delays the overfittting, the model trained over $50$ epochs still resulted missclassification of class 1.\n",
    "* I also work on using weights for the loss to put more enphasis on class 0 but this also didnt help\n",
    "* I also considered freezing and unfreezing different parts of the pretrained model such as the final cnn layer, while this looked promising initially,  the same thing happend at evaluation. \n",
    "\n",
    "For my final answer, I can say, without penalizing the model very strictly, it quickly converges on the training set within only a few epochs resulting in $85\\%$ accuracy, restricting it a bit leads to a delayed overfitting and still same reasult. using a very high tropup kills it entirely so that it cannot predict even the majority class well. I do not know for sure if being very strict and training longer would have gotten me the 100% on evaluation results or not.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
