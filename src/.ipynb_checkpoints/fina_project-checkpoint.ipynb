{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on Tue Apr 21 11:10:11 2020\n",
    "\n",
    "@author: Deeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n",
      "Requirement already satisfied: torch in /home/moselim/.conda/envs/tensorflow/lib/python3.7/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy in /home/moselim/.conda/envs/tensorflow/lib/python3.7/site-packages (from torch) (1.17.4)\n",
      "Requirement already satisfied: future in /home/moselim/.conda/envs/tensorflow/lib/python3.7/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: torchsummary in /home/moselim/.conda/envs/tensorflow/lib/python3.7/site-packages (1.5.1)\n",
      "Requirement already satisfied: torchvision in /home/moselim/.conda/envs/tensorflow/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy in /home/moselim/.conda/envs/tensorflow/lib/python3.7/site-packages (from torchvision) (1.17.4)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/moselim/.conda/envs/tensorflow/lib/python3.7/site-packages (from torchvision) (7.1.1)\n",
      "Requirement already satisfied: torch==1.5.0 in /home/moselim/.conda/envs/tensorflow/lib/python3.7/site-packages (from torchvision) (1.5.0)\n",
      "Requirement already satisfied: future in /home/moselim/.conda/envs/tensorflow/lib/python3.7/site-packages (from torch==1.5.0->torchvision) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchsummary\n",
    "! pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm # Displays a progress bar\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torchsummary import summary\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets,models, transforms\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, random_split\n",
    "from data_gen import Dataset\n",
    "from augment import Augmentations as aug\n",
    "from visualization  import visualizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset and train, val, test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "num_classes = 2\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "datacsv = pd.read_csv(\"data.csv\")\n",
    "\n",
    "cv_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------Data Augmentation--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_aug = dict(datacsv)\n",
    "data_base_aug[\"image\"] = list(data_base_aug[\"image\"])\n",
    "data_base_aug[\"label\"] = list(data_base_aug[\"label\"])\n",
    "\n",
    "rot = aug()\n",
    "for k in range(len(data_base_aug[\"image\"])):\n",
    "    result = rot.rotate_append(data_base_aug[\"image\"][k],data_base_aug[\"label\"][k])\n",
    "    datacsv = datacsv.append(result,ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------Create dataset generator-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cv in range(1):\n",
    "    x_train,x_test =tts(datacsv[\"image\"],test_size=1/6, shuffle=True)\n",
    "    partition = {'train':list(x_train),'validation':list(x_test)}\n",
    "    labels = {}\n",
    "    \n",
    "    for k in range(len(datacsv['image'])):\n",
    "        labels['%s'%datacsv['image'][k]] = datacsv['label'][k]\n",
    "        \n",
    "    #####################################################################\n",
    "    \n",
    "    training_set = Dataset(partition[\"train\"],labels)\n",
    "    training_generator = data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    validation_set = Dataset(partition['validation'], labels)\n",
    "    validation_generator = data.DataLoader(validation_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #######################################################\n",
    "    class Network(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "            self.conv2 = nn.Conv2d(6, 6, 5)\n",
    "            self.conv3 = nn.Conv2d(6, 3, (5,5))\n",
    "            self.conv4 = nn.Conv2d(3, 3, (7,4))\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.poolav = nn.AvgPool2d(2,2)\n",
    "            self.fc1 = nn.Linear(3* 42 * 28, 16)\n",
    "            self.fc2 = nn.Linear(16, num_classes)\n",
    "            \n",
    "    \n",
    "        def forward(self,x):\n",
    "            # TODO: Design your own network, implement forward pass here\n",
    "            x = self.pool(F.relu(self.conv1(x))) #3*748*500 -> 6*744*496 -> 6*372*2\n",
    "            x = self.poolav(F.relu(self.conv2(x))) #6*372*254 -> 12*368*250 -> 12*184*125\n",
    "            x = self.pool(F.relu(self.conv3(x))) #12*184*125 -> 6*180*122 -> 6*90*61\n",
    "            x = self.poolav(F.relu(self.conv4(x))) #6*90*61-> 3*84*58 -> 3*42*29\n",
    "            #print(\"Before view: %s by %s\"%(x.shape[-2],x.shape[-1]))\n",
    "            x = x.view(-1, 3 * 42 * 28)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            out = F.softmax(self.fc2(x))\n",
    "            return out\n",
    "    \n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Configure device\n",
    "    model = Network().to(device)\n",
    "    summary(model,(3,748,500))\n",
    "    criterion = nn.CrossEntropyLoss() # Specify the loss layer\n",
    "    optimizer = optim.Adam(model.parameters(),lr=0.001,momentum=0.99) # Specify optimizer and assign trainable parameters to it, weight_decay is L2 regularization strength\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(model, training_generator, num_epoch = num_epochs): # Train the model\n",
    "        print(\"Start training...\")\n",
    "        model.train() # Set the model to training mode\n",
    "        for i in range(num_epoch):\n",
    "            running_loss = []\n",
    "            accuracy = []\n",
    "            for batch, label in tqdm(training_generator):\n",
    "                batch = batch.to(device)\n",
    "                label = label.to(device)\n",
    "                optimizer.zero_grad() # Clear gradients from the previous iteration\n",
    "                pred = model(batch) # This will call Network.forward() that you implement\n",
    "                loss = criterion(pred, label) # Calculate the loss\n",
    "                running_loss.append(loss.item())\n",
    "                correct = (torch.argmax(pred,dim=1)==label).sum().item()\n",
    "                accuracy.append(correct/batch_size)\n",
    "                loss.backward() # Backprop gradients to all tensors in the network\n",
    "                optimizer.step() # Update trainable weights\n",
    "            print(\"Epoch {} loss:{}\".format(i+1,np.mean(running_loss))) # Print the average loss for this epoch\n",
    "            print(\"Epoch {} Accuracy:{}\".format(i+1,np.mean(accuracy)))\n",
    "        print(\"Done!\")\n",
    "        try:\n",
    "            torch.save(model, \"Trained_model/teeth_model.pth\")\n",
    "            print(\"Model saved!\")\n",
    "        except:\n",
    "            print(\"Could not save model\")\n",
    "    \n",
    "    def evaluate(model, validation_generator): # Evaluate accuracy on validation / test set\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.set_grad_enabled(True): # Do not calculate grident to speed up computation\n",
    "            for batch, label in tqdm(validation_generator):\n",
    "                batch = batch.to(device)\n",
    "                label = label.to(device)\n",
    "                pred = model(batch)\n",
    "                correct += (torch.argmax(pred,dim=1)==label).sum().item()\n",
    "                total+=batch_size\n",
    "        acc = correct/total\n",
    "        print(\"Evaluation accuracy: {}\".format(acc))\n",
    "        return acc\n",
    "        \n",
    "    train(model, training_generator, num_epochs)\n",
    "    print(\"Evaluate on validation set...\")\n",
    "    result = evaluate(model, validation_generator)\n",
    "    cv_results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 90.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean accuracy: %s\"%(np.mean(cv_results)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAABSCAYAAADKHjGZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAES0lEQVR4nO3dzWtcdRTG8edJE4xUs+imC4WARUFqVUR8RVy4sFbUQldWUaGCKIW67KIbY/8BUSmII6mlVBQVFxYFRXxDF0KtQdEoklYs4qbENhRt4biYKBLI5Ixn7G+C3w8EwuRw7uFmntzJnd/c64gQgH9vpPUAwGpHiIAiQgQUESKgiBABRYQIKCJEQFHTENleZ/tN2wu2j9ne3nKeYWJ7p+0vbP9ue7r1PMPE9gW2O4vPmVO2v7R9V6t5RltteNHzkv6QtF7StZLetn00Ir5uO9ZQOCFpr6Q7JV3YeJZhMyrpJ0m3SzouaYukV21vioi58z2MW61YsL1W0klJV0XE7OJjByT9HBG7mww1hGzvlXRpRDzSepZhZvsrSU9FxOvne9stX85dIencXwFadFTSxkbzYJWyvV7d51OTVzAtQ3SRpN+WPDYv6eIGs2CVsj0m6aCk/RHxbYsZWobotKSJJY9NSDrVYBasQrZHJB1Q9//qna3maBmiWUmjti//x2PXqNEhGauLbUvqqHtSaltEnG01S7MQRcSCpDckTdlea/tWSfep+5flf8/2qO1xSWskrbE9brv12dRhsk/SlZLuiYgzLQdp/WbrE+qevv1V0iFJj3N6+297JJ2RtFvSg4vf72k60ZCwPSnpMXXfFvnF9unFrweazMOH8oCa1kciYNUjREARIQKKCBFQtNIp0/RZhyk5Vffh4fzSpve3HM8VPvNkuqd2JQddwVY7vW/eSu7G0FR+gJhJFr6W76nB7Bspv2+sDam6af2Q3vqUXkjV/ejsPpQinl1233AkAooIEVBEiIAiQgQUESKgiBABRYQIKCJEQBEhAooIEVDU8/NEO058kF6+0bnk5dwGNZ1tmdbfR6JiIEtbvHEkv9UbkqXTn+cHmLwxVzd3f7pl6NBglv30sSTKyV9e+Lo+BjiSKxvQ84YjEVBEiIAiQgQUESKgiBABRYQIKCJEQBEhAooIEVDU80IlL23N36Chk1yJcKyPt4knk3Xu4332QV3wNb65JV3rc5/mevaxb87O5erG9Eq6Z/dKzoPwdLryIS+9u84yYiHd827dkar7xO+le873+NVwJAKKCBFQRIiAIkIEFBEioIgQAUWECCgiREARIQKKCBFQ1Pv+RB/fm27k8eQFJ/q6Tkiu5/V9dByc3FIeSYrvkoXel+45dvNNucLPLkv3HBTHfLr2hCZSdd97Nt1zg3I7/PC2/eme0sPL/oQjEVBEiIAiQgQUESKgiBABRYQIKCJEQBEhAooIEVDU89YqJ/u4Rca6ZOWmd7IdpZnNL6bqDurRdM/tA7q1Sj+3D5lJVl7dx4VKbutsTtV9dOTddE89N6h90+njcjA7ci2VHy27cWevhCMp5ri1CvCfIURAESECiggRUESIgCJCBBQRIqCIEAFFhAgoIkRAUc9lPwBWxpEIKCJEQBEhAooIEVBEiIAiQgQU/QlBGt4iz/70HgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis=visualizer()\n",
    "vis.plot_filter(layer=model.conv1,single_channel = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAABSCAYAAADKHjGZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEXElEQVR4nO3dT4hVZRjH8d9Pp1RMFyHZYqg2RlJRSLsQqYhKiBZCizKqVX8QwkUxkFGWbQqiFuZKyCQC6c/KRVA5kEtJDaOwTWqWiRA2DvYPnhb3FiF4fE7P5HsvfT8wMJx5eM9zD+c3586577zHESEA/9681g0A444QAUWECCgiREARIQKKCBFQRIiAoqYhsn257Q9tz9o+YvuBlv2MEtsbbO+z/avtt1r3M0psL7C9fXjOzNg+YPueVv1MtNrx0FZJv0laLulmSbttH4yIL9u2NRK+l7RF0l2SFjXuZdRMSDomaY2ko5LWStpl+8aI+PZiN+NWMxZsL5b0k6QbIuLwcNtOSccjYqpJUyPI9hZJkxHxSOteRpntLyRtjoj3L/a+W76du1bSH38FaOigpOsb9YMxZXu5BudTk3cwLUN0maSfz9l2WtKSBr1gTNm+RNI7knZExNctemgZojOSlp6zbamkmQa9YAzZnidppwZ/V29o1UfLEB2WNGF7xT+23aRGl2SMF9uWtF2Dm1LrIuL3Vr00C1FEzEr6QNKLthfbvlXSfRr8Zvnfsz1he6Gk+ZLm215ou/Xd1FGyTdJKSfdGxNmWjbT+sPVJDW7fnpT0rqQnuL39t02SzkqakrR++P2mph2NCNtXS3pMg49FTtg+M/x6sEk//FMeUNP6SgSMPUIEFBEioIgQAUWdt0x3WOm7DktOOFW37sr8jYz8PY/cvoej9ik+/x4fzx8bHdqRq9v7dnrIWPlJrvCr9JBSvwPZMYrTx+ajN65J1d391PoeDbyUqoo+LzfOf95wJQKKCBFQRIiAIkIEFBEioIgQAUWECCgiREARIQKKCBFQ1Pn/RFZ++oZ8JFcXr6SH1Hdbc3WTz6eHDL0wJ1NbjveYEjWZnV7yco8pUc8uThbOpsfUXE372Z0/b3woVxfTPU7FvcnCmfzLjY7pYlyJgCJCBBQRIqCIEAFFhAgoIkRAESECiggRUESIgKLuFVB7LDgxk1xVZIl+yA6pwQPQLuzUZwfSIy5bPTcLlajHbI47/Xqq7qGpS9N7f/j0dK7wzV3pMbs+le/jaI9jc5VuSdWtPbYvvf/rNubqXnsvPaS6FrjhSgQUESKgiBABRYQIKCJEQBEhAooIEVBEiIAiQgQUESKgqHPaz6rb89M39m9OFq7usRhHct2MXk8nmqvnEy3KH5s4myvdtCrf2nOfL0jVLdCj6TGlbXNzbHxb+tg8HdOpuld77H/az6Tq1qjHojk8nwj47xAioIgQAUWECCgiREARIQKKCBFQRIiAIkIEFE10/XD/nvzsAn2a+7A7nFt8ZGBZquqkTuWH7PGSOv2SXzjD9+cWKon9PZrLzi34OD+k7uhR2yG27UnXbtQ3uTG1It/AFcmZLj/2mT1zflyJgCJCBBQRIqCIEAFFhAgoIkRAESECiggRUESIgCJCBBR1P58IwAVxJQKKCBFQRIiAIkIEFBEioIgQAUV/AuHL5HY7Hme3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis.plot_filter(layer=model.conv2,single_channel = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAABSCAYAAADKHjGZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEUElEQVR4nO3dS4iVdRjH8d9PR5ywZtFmjOxCYNBFrBYRtchWVlSiRtFlYZukctVCLERQJqJFu0oIjERMMlRcRK6yRYsIuxgW4SKqoeiy6KImXeBpcaYIYY7P23Oa/zn0/cDAcObheZ955/zmnXnP/32PI0IA/r15rQcARh0hAooIEVBEiIAiQgQUESKgiBABRU1DZPt82wdsn7L9he37W84zTGxvsH3E9q+2X249zzCxvdD2jpnnzAnbH9q+rdU8Y602PON5Sb9JmpR0jaTXbR+NiI/bjjUUvpY0JWmlpHMazzJsxiRNS7pZ0peSbpe01/ayiPh8rodxqxULthdJ+kHS1RFxfOaxXZK+iohNTYYaQranJC2JiHWtZxlmtj+StDUi9s31tlv+OXe5pD/+CtCMo5KuajQPRpTtSfWeT03+gmkZonMl/XzGYz9JOq/BLBhRthdI2i1pZ0R82mKGliE6KWnijMcmJJ1oMAtGkO15knap93/1hlZztAzRcUljtpf+47HlanRIxmixbUk71DsptTYifm81S7MQRcQpSfslbbO9yPZNklap95vlf8/2mO1xSfMlzbc9brv12dRhsl3SFZLujIjTLQdp/WLro+qdvv1O0h5Jj3B6+2+bJZ2WtEnSgzOfb2460ZCwfYmk9eq9LPKN7ZMzHw80mYeL8oCa1kciYOQRIqCIEAFFhAgo6n/K1GvSZx18cDJXeNf2bEuFnSu8Nd1SeiOSTc/Cyu+bjcnC6/OjxUu5zXtxuqVihwayb+wD6X0zf+PqVN3pZ/LbX5D8Nlbckj+p9tbh2ZtyJAKKCBFQRIiAIkIEFBEioIgQAUWECCgiREARIQKKCBFQdJbriTzwi40u7NDyzeTyja0dLmx8ReODWdpyaE/6G9k3dV+qbu3bXQbIFq5Kt4w4OJglUdvyP2RvydV91mGyy5J1Ha+lY9kP8F8hREARIQKKCBFQRIiAIkIEFBEioIgQAUWECCjqu2Lh2xvyN+NY/E6u7trszUckvZ/e+pJ0T2l6MCsWOqzmWDGRKz38Y4cBlj+XKpteeEe65UVHLh3QjUoWpPdN7M/dh95r1qW3H7Ez1zPdUQrNfoMbjkRAESECiggRUESIgCJCBBQRIqCIEAFFhAgoIkRAESECivq+P9HkyXs7tNqbqvrgxQ4t0+b+TaPf67Bm5JfsApOHnk739LEnkpUr0z0HdVea0O507burk/vmtYs7TZDiDk/GPi05EgFFhAgoIkRAESECiggRUESIgCJCBBQRIqCIEAFFfVcs6Nir+U5O1saV+Zb+JFf4+AvpnvFsurSv63Z1eH3/gtyr8t8/uaLDBDemqkKHOvRc2qF2dpu33JOufUrjqbrH7s6/RYyzN8NZlm4p6eFZv8KRCCgiREARIQKKCBFQRIiAIkIEFBEioIgQAUWECCgiREBR3/cnAnB2HImAIkIEFBEioIgQAUWECCgiREDRnyXczdoxaitTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis.plot_filter(layer=model.conv3,single_channel = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALoAAABSCAYAAADjGc4eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEZklEQVR4nO3dS2hVVxQG4P/XiFprBiIVQQ0iFcRohHbQ6sCJ4ANFqaCQIpSMHAg6UYQ6SNpMSp0WW4qhJRRE8AHiIOJIFDpIQVMECRSibdWBD3zE+CgsB/cKEtzryO3We85d/wdncnfO3vsc/pxc193uSzODSKub0uwJiLwPCrqEoKBLCAq6hKCgSwgKuoSgoEsI2YNOcg7JUyTHSV4n2Z17jKoiuYfkMMlnJH9p9nzKguR0kkfreXlE8jLJjTnHaMvZWd0PAJ4DmAdgFYCzJK+Y2dV3MFbV3ATQD2A9gJlNnkuZtAH4G8BaADcAbAJwnOQKMxvLMQBzfjJKchaA+wA6zWy0/toggH/N7GC2gSqOZD+ABWb2VbPnUlYkRwD0mdmJHP3lfuuyFMB/r0JedwXA8szjSAsjOQ+1LGV7F5A76B8CeDjptQcAZmceR1oUyWkAfgPwq5ldy9Vv7qA/BtA+6bV2AI8yjyMtiOQUAIOo/RtvT86+cwd9FEAbyY9fe60LGf8ESWsiSQBHUStibDezFzn7zxp0MxsHcBLANyRnkVwDYCtqv6XhkWwjOQPAVABTSc4g+S4qX1V0BMAyAFvMbCJ772aW9QAwB8BpAOOolYq6c49R1QNALwCbdPQ2e17NPgB01O/FU9Te/r46vsw1RtbyokhZaQmAhKCgSwgKuoSgoEsICrqE4NZwf6yVfN5oN7/3e7YDyaZt9Cs9pw30O28+Mn1vYI1P/9Cu1W77t4OXSn9v/nRys7Kn4OQBp43+pZulb7ye6BKCgi4hKOgSgoIuISjoEoKCLiG45cUbTjWHSJcPaz/glBCH/FMrwSkhmlN5BABsPpVu6/6iwQmVx0mnDFi4iJBL0m2jjS9A1BNdQlDQJQQFXUJQ0CUEBV1CUNAlBPf/jM51aoR9fX6p50xvum3Iq1sCsIX/Y/nfe8K76Rqizb3on9y/M9nUc2ife+qA7S//vVk7LX1vLhTsYvFT+vL4uZ85W5Fe9aonuoSgoEsICrqEoKBLCAq6hKCgSwgKuoTg1tE5cjtdD+2a73a80Fmq+s8xf1K2swq7AKQ/YzhccG7XB+m2HU/8WvG9SuyQkL43xQtt0z/RU3DpA9oFQKJT0CUEBV1CUNAlBAVdQlDQJYSCr3ZJl4mIDr/ng2PJpuvf+acuqkIJbcjZ5mBDwXJSt/mzgpF/L/+9+fRwurz4xxH33FX2V7LtcsEmo1B5UaJT0CUEBV1CUNAlBAVdQlDQJQQFXUJwd9N1VltiGF+7HX/S7dQ8W+Br2Z+uT7fdKfgY4LzTtq6x6ZTL8P5k08yCUyecNhZsw+ylSk90CUFBlxAUdAlBQZcQFHQJQUGXENzy4kc4l2y7NdbpdjzSsTjZtnJ5wayuFrSXwEOngvjzoF8+9Upo68q/QvktpK9/wr16AN4Xfe1tfFw90SUEBV1CUNAlBAVdQlDQJQQFXUJQ0CWEgu0uRFqDnugSgoIuISjoEoKCLiEo6BKCgi4hvAStK+NgkSKfyAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis.plot_filter(layer=model.conv4,single_channel = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can only visualize layers which are convolutional\n"
     ]
    }
   ],
   "source": [
    "vis.plot_filter(layer=model.fc1,single_channel = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
